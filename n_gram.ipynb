{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d84594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7cece93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ce23c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ekstrak_teks_dari_pdf(path):\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text(x_tolerance=2, y_tolerance=2)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e11d18b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsevier.com/locate/displa\n",
      "A unified architecture for super-resolution and segmentation of remote\n",
      "✩\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "# File yang sama\n",
    "teks_1 = ekstrak_teks_dari_pdf(\"./jurnal/paper1.pdf\")\n",
    "print(teks_1[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5591b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displays 84 2024 102800\n",
      "Contents lists available at ScienceDirect\n",
      "Displays\n",
      "journal homepage wwwelseviercomlocatedispla\n",
      "A unified architecture for superresolution and segmentation of remote\n",
      "✩\n",
      "sensing images based on similarity feature fusion\n",
      "Lunqian Wanga Xinghua Wanga Weilin Liua Hao Dinga Bo Xiaa Zekai Zhangb\n",
      "Jinglin Zhangbc∗  Sen Xud\n",
      "aDepartment of Information Science and Engineering Linyi University Linyi 276000 China\n",
      "bDepartment of Control Science and Engineering Shandong University Jinan 250061 China\n",
      "cShandong Research Institute of Industrial Technology Jinan 250013 China\n",
      "dDepartment of Information Science and Engineering YanCheng Institute of Technology Yancheng 224000 China\n",
      "A R T I C L E I N F O A B S T R A C T\n",
      "MSC The resolution of the image has an important impact on the accuracy of segmentation Integrating super\n",
      "41A05 resolution SR techniques in the semantic segmentation of remote sensing images contributes to the\n",
      "41A10 improvement of precision and accuracy especially when the images are blurred In this paper a novel\n",
      "65D05\n",
      "and efficient SR semantic segmentation network SRSEN is designed by taking advantage of the similarity\n",
      "65D17\n",
      "between SR and segmentation tasks in feature processing SRSEN consists of the multiscale feature encoder\n",
      "Keywords the SR fusion decoder and the multipath feature refinement block which adaptively establishes the feature\n",
      "Deeplearning\n",
      "associations between segmentation and SR tasks to improve the segmentation accuracy of blurred images\n",
      "Semanticsegmentation\n",
      "Experiments show that the proposed method achieves higher segmentation accuracy on fuzzy images compared\n",
      "Superresolution\n",
      "to stateoftheart models Specifically the mIoU of the proposed SRSEN is 3–6 higher than other\n",
      "Remotesensingimage\n",
      "stateoftheart models on lowresolution LoveDa Vaihingen and Potsdam datasets\n",
      "1 Introduction natural images the semantic segmentation of remote sensing images\n",
      "has special characteristics and challenges such as the existence of\n",
      "Remote sensing data has been widely used in the field of computer a large number of tiny targets and complex terrain scenes which\n",
      "vision with the rapid development of artificial intelligence remote requires that the segmentation method and the results should be fine\n",
      "sensingdata hasawiderange ofapplicationsinsemanticsegmentation enough However remote sensing images usually have different spatial\n",
      "which covers many fields including geographic information systems resolutions due to different sensors When acquiring remote sensing\n",
      "environmental monitoring agriculture urban planning and so on 1–\n",
      "images weather angle and illumination can affect the quality of im\n",
      "6 Semantic segmentation is a fundamental yet challenging task in\n",
      "ages The reduction of image resolution can result in the loss of spatial\n",
      "image scene understanding which has important research value in\n",
      "information such as the shape of edges 1314 Semantic segmenta\n",
      "medicine 78 geological science 910 remote sensing 1112 and\n",
      "tion techniques based on deep learning often require remote sensing\n",
      "other fields Compared to other computer vision tasks such as image\n",
      "images with high spatial resolution and the loss of spatial information\n",
      "classification or object detection semantic segmentation is computa\n",
      "will reduce the segmentation results However the enhancement of\n",
      "tionally demanding because it involves predicting the category of each\n",
      "hardware technology has limitations in various aspects and using\n",
      "pixel Image segmentation remains a challenging problem due to rich\n",
      "superresolution SR technology to enhance the resolution of remote\n",
      "intraclass variation contextual variation and ambiguity caused by\n",
      "sensing images is an effective method Rapidly evolving techniques in\n",
      "occlusion and low image resolution\n",
      "deep learning have allowed SR methods to be actively explored and\n",
      "Semantic segmentation based on remote sensing images is an im\n",
      "achieve stateoftheart performance Various SR networks based on\n",
      "portant pendant application in the field of image segmentation which\n",
      "CNN 15–18 and Transformer 19–21 are emerging The design of\n",
      "has a wide range of needs and practical application value especially\n",
      "lightweight SR networks 22–24 also has been rapidly advanced with\n",
      "in scenarios such as disaster assessment crop yield estimation and\n",
      "the increasing number of parameters and computing time in networks\n",
      "land change monitoring Compared with the semantic segmentation of\n",
      "✩\n",
      "This paper was recommended for publication by Prof Guangtao Zhai\n",
      "∗\n",
      "Corresponding author\n",
      "Email addresses 210854002032lyueducn L Wang jinglinzhangsdueducn J Zhang\n",
      "httpsdoiorg101016jdispla2024102800\n",
      "Received 31 March 2024 Received in revised form 13 July 2024 Accepted 17 July 2024\n",
      "Available online 20 July 2024\n",
      "01419382© 2024 Elsevier BV All rights are reserved including those for text and data mining AI training and similar technologiesL Wang et al Displays 84 2024 102800\n",
      "LR remote sensing images In SRSEN the multiscale feature\n",
      "encoder generates features at different scales containing seman\n",
      "tic information while using subpixel convolution to generate\n",
      "HR features containing rich lowfrequency information The SR\n",
      "fusion decoder achieves SR of the features while extracting\n",
      "deep semantic information and aggregates the multiscale fea\n",
      "tures provided by the encoder to ensure that the lowfrequency\n",
      "information is fully utilized\n",
      "2 Feature association and refinement In the decoder the CFSA\n",
      "is proposed to achieve remote dependency of features based on\n",
      "local associations which avoids the limitation of the original\n",
      "selfattention that ignores the local information associations In\n",
      "addition the MFRB is proposed in the tail of the network for\n",
      "refining and filtering the complex features after aggregation to\n",
      "further improve the segmentation accuracy\n",
      "3 Experimental proof We experimentally demonstrate that\n",
      "SRSEN has outstanding semantic segmentation performance on\n",
      "LR remote sensing images The segmentation results of SRSEN\n",
      "on the LR LoveDA Potsdam and Vaihingen datasets are 3–6\n",
      "higher than other advanced segmentation models In addition\n",
      "SRSEN also has a better SR effect and excellent segmentation\n",
      "precisions at different SR scales\n",
      "Fig 1 The figure shows that LR images significantly reduce the accuracy of the\n",
      "semantic segmentation network compared to the HR image The proposed SRSEN with\n",
      "the SR function can substantially improve the segmentation accuracy on the LR image\n",
      "2 Relate work\n",
      "As a result the application of SR technology in various fields has been 21 Semantic segmentation\n",
      "advanced However lowresolution LR images first perform the SR\n",
      "task and then implement semantic segmentation will take up a lot\n",
      "The development of deep learning has resulted in CNNs being\n",
      "of computational resources and inference time 25 In addition the\n",
      "widely applied in the field of computer vision Since the introduction of\n",
      "segmentation task requires multiple downsampling of the image to\n",
      "CNN to semantic segmentation tasks by FCN 32 CNNbased methods\n",
      "obtain local features of the target but the downsampled features are\n",
      "have dominated the semantic segmentation task in the remote sensing\n",
      "not favorable for the SR task It is a challenge to build a semantic\n",
      "field 33–35 However the network constructed by stacked convo\n",
      "segmentation network that can effectively realize the SR function of\n",
      "lution will destroy the resolution of the original image limiting the\n",
      "remote sensing images The network structure needs to focus on the\n",
      "segmentation accuracy To solve this problem UNet 36 an encoder–\n",
      "correlation of features in SR tasks and semantic segmentation tasks\n",
      "Through our research we found that segmentation tasks and SR decoder network has been proposed which effectively alleviates the\n",
      "tasks share similarities in the extraction and processing of features problem of spatial resolution degradation Subsequently the encoder–\n",
      "They are both more sensitive to highfrequency information in the decoder architecture became the standard structure for remote sensing\n",
      "image especially the edge parts of things The localization and under imagesegmentationnetworksAndvariousencoder–decoderbasednet\n",
      "standing of features are also essential and they affect the similarity of works were designed to solve various segmentation problems 37–39\n",
      "the reconstructed highresolution HR images and the accuracy of the Although CNNbased encoder–decoder models have achieved superior\n",
      "segmentation results In addition both tasks often utilize multiscale\n",
      "performance they still encounter bottlenecks in remote sensing image\n",
      "feature fusion to ensure comprehensive image information and multi\n",
      "segmentation tasks 40–42 CNNbased networks can only establish\n",
      "scale feature fusion methods are commonly applied in both tasks 26–\n",
      "local feature associations and lack the ability to model associations for\n",
      "31 Enhancing the resolution of feature images in segmentation net\n",
      "the global information of the whole image CNNbased networks are\n",
      "works using the SR technique can effectively mitigate the effects caused\n",
      "difficult to accurately segment LR remote sensing images due to the\n",
      "by LR images In this paper we design a novel efficient SR semantic\n",
      "limited sensing field\n",
      "segmentation network SRSEN with the help of the similarity between\n",
      "Recently the application of ViT 43 has solved the difficulties en\n",
      "the segmentation task and the SR task As shown in Fig 1 SRSEN\n",
      "countered by CNNbased networks Global semantic information plays\n",
      "can effectively mitigate the low segmentation accuracy caused by LR\n",
      "a crucial role in achieving accurate semantic segmentation Trans\n",
      "image segmentation networks thus achieving HR segmentation of LR\n",
      "images Selfattentive mechanism performs well in establishing remote former demonstrates powerful sequencetosequence modeling capabil\n",
      "associations of features but is often limited in the exploitation of local ities and shows outstanding performance in extracting global informa\n",
      "information We design the crossfusion selfattention CFSA which tion Transformerbased encoder–decoder models have been designed\n",
      "utilizes the dualpath structure to adaptively fuse local and remote for segmentation task 44–46 However the Transformerbased net\n",
      "associations to ensure the full application of feature information In work is prone to ignore local information and sometimes overlooks the\n",
      "additionsimplefusionmaydegradeperformanceduetothecomplexity edge features of the object thus affecting the segmentation accuracy To\n",
      "and diversity of the features extracted from the model Therefore we\n",
      "make the network acquire both local and remote information the re\n",
      "design a multiscale feature refinement block MFRB at the tail of the\n",
      "searchers adopted the hybrid structure to further improve the accuracy\n",
      "model to refine and effectively fuse features\n",
      "of remote sensing image segmentation 4748 However when the\n",
      "The contributions of this paper can be summarized as follows\n",
      "resolution of the remote sensing image is disturbed the performance of\n",
      "1 Superresolution Segmentation Network Based on the sim the segmentation networks is drastically reduced We utilize the super\n",
      "ilarity between the SR and segmentation tasks in feature pro resolution technique to achieve HR segmentation of LR images based\n",
      "cessing the SRSEN is proposed to achieve HR segmentation of on the model of encoder–decoder\n",
      "2L Wang et al Displays 84 2024 102800\n",
      "Fig 2 The overview of the SRSEN which consists of the multiscale feature encoder the SR fusion decoder and the multipath feature refinement block In the encoder RSAB\n",
      "utilizes the selfattention mechanism to establish associations when extracting features In the decoder SRTB uses the Transformerbased structure and CFSA to extract semantic\n",
      "features while improving the accuracy of feature upsampling\n",
      "22 Superresolution technology block MFRB The encoder utilizes convolution and selfattention to\n",
      "generate multiscale features that provide the decoder with rich high\n",
      "With the development of artificial intelligence SR technology has frequency and lowfrequency information The decoder realizes the SR\n",
      "also made great progress SR techniques have been successfully applied task of the features when extracting the deep semantic features and\n",
      "in many fields such as medicine 4950 remote sensing 5152 fuses the multiscale information provided by the encoder to further\n",
      "and energy engineering 5354 Deep learningbased SR models are enhance the authenticity of the HR features and the accuracy of the\n",
      "also being iteratively updated laying the foundation for expanded segmentation The arithmetic of the overall structure is shown in Alg\n",
      "applications of SR technology 5556 upsampling methods such as 1 A detailed description of each component is given in the following\n",
      "inverse convolution 57 and subpixel convolution 58 are designed sections\n",
      "and implemented so that the feature mappings learned by the SR model\n",
      "are fitted to LR images greatly reducing the number of parameters and 31 Multiscale feature encoder\n",
      "computational complexity The introduction of attentional mechanisms\n",
      "enhances the effectiveness of SR methods RCAN 59 introduces chan In order to fully exploit the hidden information in the LR image\n",
      "nel attention to enhance the importance of features that are beneficial the encoder adopts a dualpath structure the downsampling path and\n",
      "to the SR task NLSA 60 brings significant improvement in SR results the upsampling path The downsampling path adopts the residual self\n",
      "by exploiting the intrinsic feature correlation in natural images through attentive block RSAB RSAB is shown in Fig 2 There are a total of 𝑚\n",
      "a nonlocal attention mechanism NLA Inspired by vision transform RSABs in the encoder each of which downsamples the feature image\n",
      "ers 61 selfattention has been employed in SR to capture longterm with a scale factor of 2 and passes the output to the decoder via a jump\n",
      "adaptabilitySwinIR19makestheSReffectdrasticallyimprovedwith connection The upsampling path upsamples the feature image twice\n",
      "the help of the transformer There have been researches combining with the scale factor of 2 using subpixel convolution which provides\n",
      "SR techniques with segmentation models FarSeeNet 62 uses sub sufficient lowfrequency information for the processing of HR features\n",
      "pixel convolution operation instead of bilinear interpolation for more in the decoder\n",
      "accurate upsampling to improve segmentation accuracy RCNet 63 Specifically in the encoder the input LR image is first passed\n",
      "introduces a feature calibration module for calibrating highresolution through a 3 × 3 convolution to generate 𝐹 ∈ R𝐵×𝐶×𝐻×𝑊 then\n",
      "𝑖𝑛\n",
      "features upsampled from lowresolution features DSRL 64 uses the 𝐹 is fed into the downsampling path and the upsampling path\n",
      "𝑖𝑛\n",
      "dualpath structure to achieve the combination of SR and segmentation In the downsampling path 𝐹 passes through 𝑚 cascaded RSABs to\n",
      "𝑖𝑛\n",
      "tasks generate deep feature 𝐹 ∈ R𝐵×2𝑚𝐶×𝐻∕2𝑚×𝑊∕2𝑚 containing rich semantic\n",
      "𝑒\n",
      "There are also some researches that first process the images with information this process can be expressed as\n",
      "superresolution techniques and then feed them into the segmentation\n",
      "𝐹  𝐻𝑚 𝐻𝑚−1 ⋯𝐻1 𝐹 ⋯ 1\n",
      "network 6566 SR techniques can increase the detail information of 𝑒 𝑅𝑆𝐴𝐵 𝑅𝑆𝐴𝐵 𝑅𝑆𝐴𝐵 𝑖𝑛\n",
      "images including textures edges and other minute features These where 𝐻𝑖 ⋅𝑖 ∈ 12…𝑚 denotes the 𝑖th RSAB In each RSAB the\n",
      "𝑅𝑆𝐴𝐵\n",
      "details are important for the recognition and segmentation of objects\n",
      "features are first passed through a convolutional group with residual\n",
      "and features in remote sensing images SR techniques can also reduce\n",
      "concatenation then reduced in resolution and widened in the number\n",
      "confusion between different features in the image For example build\n",
      "of channels by downsampling convolution Finally features passed\n",
      "ings and roads may be mixed in LR images but with SR processing they\n",
      "through a convolutional group with selfattention to establish inter\n",
      "can be separated more clearly thus improving the accuracy of semantic pixel associations Given an input feature 𝐹 ∈ R𝐵×𝐶×𝐻×𝑊 the process\n",
      "𝑥\n",
      "segmentation However the current research does not fully utilize the\n",
      "of RSAB can be expressed as follows\n",
      "performance of SR technology and the combination of modules or\n",
      "𝐹  𝐻 𝐹  𝐹 \n",
      "the tandem connection of tasks does not perfectly combine SR and 𝑦 𝐶𝐺 𝑥 𝑥\n",
      "segmentation And there is no SR research on remote sensing image 𝐻 𝐹   𝐻 𝐻 𝐻 𝐻 𝐹 \n",
      "𝐶𝐺 𝑥 𝐵𝑁 1×1 𝑆𝑖𝑙𝑢 3×3 𝑥\n",
      "2\n",
      "segmentation In this paper a unified model is constructed based on\n",
      "𝐹  𝐻 𝐻 𝐹 \n",
      "𝑙 𝐷𝐶 𝐶𝐺 𝑦\n",
      "the relevance of the tasks to achieve highresolution segmentation of\n",
      "𝐹  𝐻 𝐻 𝐹 \n",
      "blurred images 𝑠 𝑆𝐴 1×1 𝑙\n",
      "where 𝐹 denotes the output of the first convolution group 𝐻 ⋅\n",
      "𝑦 𝐶𝐺\n",
      "3 Method denotes the basic convolution group in RSAB 𝐻 and 𝐻 de\n",
      "1×1 3×3\n",
      "note convolution operations with kernel size of 3 and 1 respectively\n",
      "As shown in Fig 2 SRSEN consists of the multiscale feature en 𝐻 ⋅ denotes the Silu activation function 𝐻 ⋅ denotes the Batch\n",
      "𝑆𝑖𝑙𝑢 𝐵𝑁\n",
      "coder the SR fusion decoder and the multipath feature refinement Norm operation 𝐻 ⋅ denotes the downsampling convolution 𝐹 ∈\n",
      "𝐷𝐶 𝑙\n",
      "3L Wang et al Displays 84 2024 102800\n",
      "Fig 3 Illustration of the CFSA which utilizes dualpath convolution and channel attention to further stimulate selfattention SRTB utilizes convolution and channel attention to\n",
      "establish associations of local information while using the dualpath selfattention mechanism to establish remote dependencies of features\n",
      "R𝐵×2𝐶×𝐻∕2×𝑊∕2 denotes the output feature of the downsampling con 33 Crossfusion selfattention CFSA\n",
      "volution 𝐻 ⋅ indicates the selfattention mechanism 𝐹 is the out\n",
      "𝑆𝐴 𝑠\n",
      "put of RSAB In the upsampling path 𝐹 undergoes 𝑛 subpixel con For CFSA its structure is shown in Fig 3 Many works have shown\n",
      "𝑖𝑛\n",
      "volutions to increase the resolution the process is denoted as that convolution facilitates the Transformer to obtain better visual\n",
      "representations or achieve easier optimization 67–69 In addition\n",
      "𝐹  𝐻𝑛 𝐻𝑛−1⋯𝐻1 𝐹 ⋯ 3\n",
      "ℎ 𝑈𝑃 𝑈𝑃 𝑈𝑃 𝑖𝑛 HAT 20 also demonstrates that channel attention can help self\n",
      "where 𝐹 is the HR feature of the subpixel convolution output 𝐻𝑖 ⋅ attention activate more pixels The original transformer layer needs\n",
      "ℎ 𝑈𝑃\n",
      "𝑖 ∈ 12…𝑛 denote the 𝑖th subpixel convolution to first perform a series of timeconsuming operations to reshape the\n",
      "features into 𝐻𝑊 × 𝐶 which makes it impossible to apply convo\n",
      "32 Superresolution fusion decoder lution operations to the features again CFSA exploits the dualpath\n",
      "structure where one path is used to retain the original informa\n",
      "The decoder utilizes cascading SR transformer blocks SRTBs Tak tion of the input features and other path refines features and estab\n",
      "lishes local associations of features using convolutional group with\n",
      "ingadvantageofthecommonalitiesbetweenSRandsegmentationtasks\n",
      "channel attention The features processed by the two paths generate\n",
      "in feature extraction and processing SRTB implements SR decoding\n",
      "three vectors 𝑞𝑢𝑒𝑟𝑦𝑘𝑒𝑦𝑣𝑎𝑙𝑢𝑒 which contain rich local information\n",
      "while exploring the semantic information of deep features As shown in\n",
      "Fig 2 the decoder consists of 𝑚𝑛 SRTBs 𝑚 and 𝑛 denote the number Then 𝑞𝑢𝑒𝑟𝑦𝑘𝑒𝑦𝑣𝑎𝑙𝑢𝑒 establish the remote association between features\n",
      "through the selfattention mechanism This approach makes the remote\n",
      "of RSABs and subpixel convolutions respectively in the encoder The\n",
      "associations between features build on local associations which avoids\n",
      "output of the downsampling path in the encoder is used as the input\n",
      "the loss of local information and increases the richness and accuracy of\n",
      "to the decoder Simultaneously the features from the encoder are\n",
      "feature associations Specifically assuming that the input CFSA feature\n",
      "combined with the features of the corresponding size in the decoder\n",
      "is 𝐹 ∈ 𝑅𝑏×𝑐×ℎ×𝑤 the process of generating features F1 and F2 by the\n",
      "through jump links The fusion of multiscale features provides rich 𝑥\n",
      "two paths of the decoder can be expressed as follows\n",
      "highfrequency and lowfrequency information and also ensures more\n",
      "accurate localization of the features in the image during upsampling 𝐹  𝐻 𝐹 \n",
      "1 1×1 𝑥\n",
      "Specifically the output feature of the encoder 𝐹 ∈ 6\n",
      "𝑒 𝐹  𝐻 𝐻 𝐹 𝐹 \n",
      "R𝐵×2𝑚𝐶×𝐻∕2𝑚×𝑊∕2𝑚 is processed by 𝑚𝑛 cascaded SRTBs after entering 2 𝐶𝐴 𝐶𝐺 𝑥 𝑥\n",
      "the decoder Each SRTB implements the extraction and filtering of SR where𝐻 1×1 ⋅denotesthe1×1convolutionoperationTheshiftconvo\n",
      "features and semantic features and also simultaneously fuses features lution can benefit local pixels participating in shifting features among\n",
      "with the same scale from the encoder The process of the decoder can different channel groups 7071 𝐻 𝐶𝐴 ⋅ denotes the channel attention\n",
      "be expressed as  mechanism Since CFSA employs window selfattention 𝐹 1 and 𝐹 2 are\n",
      "firstpartitionedaccordingtothewindowsize𝑤2 andthencalculatedby\n",
      "𝐹 𝑑  𝐻 𝑆 𝑚 𝑅  𝑇 𝑛 𝐵 ⋯𝐻 𝑆 𝑚 𝑅  𝑇 1 𝐵 𝐻 𝑆 𝑚 𝑅𝑇𝐵 ⋯𝐻 𝑆 1 𝑅𝑇𝐵 𝐹 𝑒 𝐹 𝑠 1⋯𝐹 𝑠 𝑚𝐹 ℎ 1⋯𝐹 ℎ 𝑛 linear mapping to get 𝑄 𝑚 ∈ 𝑅 𝑏𝑚 𝑤 ℎ 2 𝑤 ×𝑤2×𝑐 and 𝑄 𝑛 ∈ 𝑅 𝑏𝑚 𝑤 ℎ 2 𝑤 ×𝑤2×𝑐  𝑄 𝑚  𝑄 𝑛\n",
      "are given learnable weights and then adaptively aggregated to generate\n",
      "4\n",
      "𝑄 𝐹 and 𝐹 are fused by feature splicing and then computed by linear\n",
      "1 2\n",
      "where 𝐹 𝑑 ∈ R𝐵×𝐶×2𝑛𝐻×2𝑛𝑊 is the output of the decoder 𝐻 𝑆 𝑖 𝑅𝑇𝐵 ⋅𝑖 ∈ mapping to obtain 𝑉 These processes can be expressed as follows\n",
      "12…𝑚  𝑛 denote the 𝑖th SRTB 𝐹𝑖⋅𝑖 ∈ 12…𝑚 represents\n",
      "𝑠 𝑄  𝐻 𝐹  𝑄  𝐻 𝐹 \n",
      "the output feature of the 𝑖th RSAB of the downsampling path in the 𝑚 𝑙𝑖𝑛𝑒 1 𝑛 𝑙𝑖𝑛𝑒 2\n",
      "encoder 𝐹𝑖⋅𝑖 ∈ 12…𝑛 denotes the output feature of the 𝑖th 𝑄  𝛼𝑄 𝛽𝑄 \n",
      "𝑚 𝑛\n",
      "ℎ\n",
      "subpixel convolution of the upsampling path in the encoder For 𝑉  𝐻 𝐻 𝐹 𝐹  7\n",
      "𝑙𝑖𝑛𝑒 𝐶𝑎𝑡 1 2\n",
      "SRTB its structure is shown in Fig 2 SRTB adopts the transformer  \n",
      "𝑄𝑄𝑇\n",
      "structure to complete the information transfer and we also design a 𝐹  𝐻 𝑉\n",
      "𝑜𝑢𝑡 𝑆𝑜𝑓𝑡𝑚𝑎𝑥 √\n",
      "novel crossfusion selfattention to establish the remote dependency of 𝑑\n",
      "features more efficiently while avoiding the loss of local information\n",
      "where 𝛼 and 𝛽 denote the learnable weights 𝐻 ⋅ denotes the linear\n",
      "𝑙𝑖𝑛𝑒\n",
      "association Assuming the input feature 𝐹  the process of a single SRTB\n",
      "𝑥 mappingoperation𝐻 ⋅denotestheSoftmaxfunction𝑑 denotes\n",
      "𝑆𝑜𝑓𝑡𝑚𝑎𝑥\n",
      "can be expressed as follows the dimension of the querykey 𝑄𝑇 denotes the key and 𝐹 denotes\n",
      "𝑜𝑢𝑡\n",
      "𝐹  𝐻 𝐻 𝐹 𝐹  the output features of the CFSA\n",
      "𝑠𝑎 𝐶𝐹𝑆𝐴 𝐵𝑁 𝑥 𝑥\n",
      "5\n",
      "𝐹  𝐻 𝐻 𝐹 𝐹 \n",
      "𝑦 𝑈𝑃 𝐶𝐺 𝑠𝑎 𝑠𝑎 34 Multipath feature refinement block MFRB\n",
      "where 𝐹 is the output feature of CFSA and 𝐻 ⋅ denotes the\n",
      "𝑠𝑎 𝐶𝐹𝑆𝐴\n",
      "CFSA operation 𝐹 is the output feature of SRTB 𝐻 represents the The decoder incorporates the features provided by the encoder\n",
      "𝑦 𝑈𝑃\n",
      "subpixel convolution utilized for upsampling containing rich lowfrequency information It achieves the extraction\n",
      "4L Wang et al Displays 84 2024 102800\n",
      "Fig 4 Illustration of the MFRB MFRB adopts the parallel structure to perform\n",
      "different types of refinement operations on the sliced features separately and then\n",
      "adaptively aggregates them\n",
      "of deep semantic information and reconstruction of highfrequency\n",
      "features However the simple fusion of these features by addition\n",
      "will reduce segmentation accuracy 72–74 Therefore we design a\n",
      "multipath feature refinement block MFRB to make these features\n",
      "fuse more effectively and improve the segmentation accuracy The\n",
      "structure of MFRB is shown in Fig 4 The features are split into four\n",
      "groups processed by convolution and attention mechanisms and then Fig 5 Comparison of feature maps of several models on the LR Vaihingen dataset\n",
      "multiplied by elementwise product and adaptive fusion Specifically From the above image it can be observed that SRSEN reconstructs more texture\n",
      "given the input feature 𝐹 ∈ 𝑅𝐵×𝐶×𝐻×𝑊 it is first processed by a information of the image which significantly improves the segmentation accuracy\n",
      "𝑥\n",
      "1 × 1 convolution and then split equally into four parts at the channel\n",
      "𝐶\n",
      "dimension named 𝐹  𝐹  𝐹  𝐹 ∈ 𝑅 𝐵× 4 ×𝐻×𝑊  Then 𝐹  𝐹  𝐹  and Algorithm 1 Process of SRSEN\n",
      "1 2 3 4 1 2 3\n",
      "𝐹 are processed by different convolutions or attention mechanisms\n",
      "4 Input I  the LR input images\n",
      "LR\n",
      "𝐹̃  𝑐  𝐹   F in ← shallow feature is extracted by 3×3 convolution\n",
      "𝑥 1 𝑥\n",
      "F  F ← Multiscale feature encoders\n",
      "𝐹 𝐹 𝐹 𝐹  𝑠𝑝𝑙𝑖𝑡𝐹̃  e h\n",
      "1 2 3 4 𝑥 for int i1 to m do\n",
      "𝐹̃  𝑢 𝐹  F ← RSAB Eqs 2\n",
      "1 1 1 e\n",
      "8\n",
      "𝐹̃  𝑢 𝑓 𝐹  end for\n",
      "2 2 3×3 2\n",
      "for int j1 to n do\n",
      "𝐹̃  𝑢 𝑓 𝐹 \n",
      "3 3 𝐶𝐴 3 F ← Subpixel convolutions Eqs 3\n",
      "h\n",
      "𝐹̃  𝑢 𝑓 𝐹  end for\n",
      "4 4 𝑆𝐴 4\n",
      "F ← SR fusion decoder\n",
      "where 𝑐 ⋅ denotes 1 × 1 convolution 𝑢 ⋅𝑖 ∈ 1234 denotes 1 × 1 d\n",
      "1 𝑖\n",
      "for int i1 to mn do\n",
      "convolution used to expand the number of channels and interactive\n",
      "F ← SRTB Eqs 5\n",
      "information 𝑠𝑝𝑙𝑖𝑡⋅ denotes the operation in which the feature is split y\n",
      "end for\n",
      "on the channel dimension 𝑓 ⋅ denotes 3 × 3 convolution 𝑓 ⋅ and\n",
      "3×3 𝐶𝐴\n",
      "I ← Multipath feature refinement block Eqs 8Eqs 10\n",
      "𝑓 ⋅ denote the channel attention and spital attention respectively HR\n",
      "𝑆𝐴\n",
      "𝐹̃ ∈ 𝑅𝐵×𝐶×𝐻×𝑊𝑖 ∈ 1234 denotes the multiscale features To I out ← Segmentation predicts I HR\n",
      "𝑖\n",
      "achieve spatial interaction of feature information\n",
      "𝐹̃\n",
      "is multiplied\n",
      "Output I\n",
      "out\n",
      " HR segmentation result of SRSEN\n",
      "1\n",
      "element by element with\n",
      "𝐹̃\n",
      "\n",
      "𝐹̃\n",
      "\n",
      "𝐹̃\n",
      "respectively to obtain the features\n",
      "2 3 4\n",
      "𝑌 ∈ 𝑅𝐶×𝐻×𝑊𝑖 ∈ 1234\n",
      "𝑖\n",
      "𝑌  𝐹̃ ∗ 𝐹̃ MFRB realizes spatial information interaction and aggregation of\n",
      "1 1 2\n",
      "features with fewer parameters and computational complexity\n",
      "𝑌  𝐹̃ ∗ 𝐹̃ 9\n",
      "2 1 3\n",
      "𝑌  𝐹̃ ∗ 𝐹̃\n",
      "3 1 4 4 Experiments\n",
      "this type of spatial interaction avoids the calculation of quadratic\n",
      "complexity Then the features are given weights and aggregated The 41 Datasets\n",
      "process is represented as follows\n",
      "ISPRS Vaihingen 2D Dataset The ISPRS Vaihingen 2D dataset is a\n",
      "𝑌  𝛼 𝑌 𝛽 𝑌 𝛾 𝑌  10\n",
      "1 2 3 benchmarkdatasetofaerialremotesensingimageslabeledbytheInter\n",
      "where 𝑌 denotes the features after adaptive aggregation 𝛼 𝛽 𝛾 are national Society for Photogrammetry and Remote Sensing ISPRS The\n",
      "learnable weights and 𝛼  𝛽  𝛾  1 Finally the dimension of 𝑌 is Vaihingen dataset consists of 33 very fine spatial resolution TOP image\n",
      "reduced to the initial number of channels by 1 × 1 convolution tiles at an average size of 2494 × 2064 pixels Each TOP image tile has\n",
      "5L Wang et al Displays 84 2024 102800\n",
      "Fig 6 Visualization results of the Vaihingen test set The first column denotes the HR RGB images The second column represents the input LR images GT denotes ground truth\n",
      "On LR remote sensing images the segmentation of SRSEN with SR module is more effective\n",
      "Table 1\n",
      "Quantitative comparison results on the LR Vaihingen test set with stateoftheart networks The table shows F1 scores for\n",
      "five categories The best values in the column are in bold\n",
      "Method Impsurf Building LowVeg Tree Car mF1 OA mIoU\n",
      "CMT 8168 8349 7192 8371 6428 7504 8005 6280\n",
      "EdgeViT 7903 8094 7046 8112 5556 7237 7618 6052\n",
      "BANet 8016 8288 7133 8161 6058 7383 7833 6143\n",
      "DCSwin 8634 8941 7805 8565 6662 8036 8429 6827\n",
      "UNetformer 8248 8420 7224 7970 6178 7787 8060 6326\n",
      "SRSENOurs 8705 8912 7734 8615 7122 8212 8470 7015\n",
      "Table 2\n",
      "Quantitative comparison results on the LR Potsdam test set with stateoftheart networks The table shows F1 scores for five\n",
      "categories The best values in the column are in bold\n",
      "Method Impsurf Building Lowveg Tree Car mF1 OA mIoU\n",
      "CMT 8088 8149 6992 6171 7728 7526 7493 6274\n",
      "EdgeViT 7861 7955 6835 6029 7352 7406 7235 6066\n",
      "BANet 8008 8196 6985 6202 7686 7583 7471 6292\n",
      "DCSwin 8352 8865 7410 7139 8241 8012 7980 6711\n",
      "UNetformer 8038 8359 7066 6326 7757 7691 7530 6314\n",
      "SRSENOurs 8429 8740 7688 7452 8545 8211 8012 6999\n",
      "Table 3\n",
      "Quantitative comparison results on the LR LoveDA val set with stateoftheart networks We train on the training set and\n",
      "test on the validation set The table shows IoU scores for seven categories The best values in the column are in bold\n",
      "Method Background Building Road Water Barren Forest Agricultural mF1 OA mIoU\n",
      "CMT 9929 6492 4451 5783 6737 3297 5023 5959 6075 4572\n",
      "EdgeViT 8391 5160 3370 4153 5356 1539 2789 5835 6236 4394\n",
      "BANet 9552 7021 5513 6375 6851 2687 4210 6044 6516 4655\n",
      "DCSwin 9886 6297 5720 6280 7349 3095 4754 6197 6139 4816\n",
      "UNetformer 9627 6341 5664 6053 7027 3116 4728 6094 6171 4663\n",
      "SRSENOurs 9483 7196 5505 6493 7385 3679 4626 6338 6735 4908\n",
      "three multispectral bands near infrared red green as well as a digital ISPRS Potsdam 2D Dataset The ISPRS Potsdam 2D dataset is a\n",
      "surface model DSM and normalized digital surface model NDSM benchmark dataset of aerial remote sensing image labels provided by\n",
      "the ISPRS The Potsdam dataset contains 38 very fine spatial resolution\n",
      "with a 9 cm ground sampling distance GSD The dataset contains six\n",
      "TOP image tiles GSD 5 cm at a size of 6000 × 6000 pixels and\n",
      "types of landcover categories namely impervious surfaces imp surf\n",
      "involves the same category information as the Vaihingen dataset Four\n",
      "buildings lowvegetationlow vegtrees carsand clutterbackground\n",
      "multispectral bands red green blue and nearinfrared as well as\n",
      "clutterInourexperimentsonlytheTOPimagetilesareusedwithout\n",
      "the DSM and NDSM are provided in the dataset Similarly only three\n",
      "the DSM and NDSM The image tiles are cropped into 1024 × 1024 bands red green blue are utilized and the original image tiles are\n",
      "pixels patches cropped into 1024 × 1024 pixels patches in the experiments\n",
      "6L Wang et al Displays 84 2024 102800\n",
      "Table 4\n",
      "Comprehensive comparison of networks on three LR datasets The complexity and speed are measured by the 256 × 256 input\n",
      "and ×4 SR on a single NVIDIA GTX 3090\n",
      "Methods FPS Params FLOPs LoveDa Potsdam Vaigingen\n",
      "mIou MeanF1 mIou MeanF1 mIou MeanF1\n",
      "CMT 56 89M 519G 457 596 627 749 628 751\n",
      "EdgeViT 66 66M 403G 439 584 607 724 605 724\n",
      "BANet 59 127M 537G 466 604 629 747 614 738\n",
      "DCSwin 20 301M 1006G 482 620 671 798 683 804\n",
      "UNetFormer 60 117M 475G 463 609 631 753 632 759\n",
      "SRSEN 18 88M 1052G 491 634 700 821 702 821\n",
      "Fig 7 Visualization results of the Potsdam test set The first column denotes the HR RGB images The second column represents the input LR images GT denotes ground truth\n",
      "The results of segmentation by SRSEN are more precise on the LR Potsdam dataset\n",
      "LoveDA Dataset The LoveDA 75 dataset contains 5987 fine segmentation results bilinear interpolation is inserted into the above\n",
      "resolution optical remote sensing images GSD 03 m at a size of segmentationmodelsasanupsamplingmoduleWeutilizethePyTorch\n",
      "1024 × 1024 pixels and includes 7 landcover categories building road framework and experiment on a single NVIDIA GTX 3090 GPU To\n",
      "water barren forest agriculture and background Specifically 2522 accelerate the convergence process we utilize the Adam optimizer to\n",
      "images are used for training 1669 images for validation and the offi facilitate the training of all models in the experiments while setting the\n",
      "cially provided 1796 images for testing The dataset encompasses two base learning rate to 6e−4 Additionally we adopt a cosine strategy to\n",
      "scenes urban and rural which are collected from three cities Nanjing dynamically adjust the learning rate throughout the training process\n",
      "Changzhou and Wuhan in China Therefore considerable challenges\n",
      "are brought due to the multiscale objects complex background and\n",
      "43 Evaluation metrics\n",
      "inconsistent class distributions In this work we preprocess the dataset\n",
      "by cropping all the images and labels to 1024 × 1024 pixels\n",
      "The evaluation metrics used in our experiments included two major\n",
      "categories The first one is to evaluate the accuracy of the network\n",
      "42 Implementation details\n",
      "including overall accuracy OA mean F1 score mF1 mean pixel\n",
      "accuracymAccandmeanintersectionoverunionmIoUThesecond\n",
      "To obtain LR datasets we downsample the images from the LoveDa\n",
      "oneistoevaluatethescaleofthenetworkincludingtheframespersec\n",
      "Potsdam and Vaihingen datasets with random fuzzy kernels while the\n",
      "ond FPS to evaluate the speed and the number of model parameters\n",
      "resolution of the labels remains unchanged The subsequent test results\n",
      "M to evaluate the memory requirement To guarantee impartial and\n",
      "for the network are based on the LR datasets We obtained ×2 LR\n",
      "efficient comparison experiments we standardize the input image size\n",
      "datasets and ×4 LR datasets by downsampling the three datasets with\n",
      "to 1 batch size 3 channels and 256 × 256 pixels\n",
      "2 and 4 scales respectively We train the models using the obtained LR\n",
      "LoveDa LR Potsdam and LR Vaihingen datasets Then we use the HR\n",
      "44 Results on the Vaihingen and Potsdam dataset\n",
      "segmentation results output by the models to compare with the labels\n",
      "and calculate the loss We have selected three classic segmentation\n",
      "models and the latest lightweight ViT models for comparison UNet The ISPRS Vaihingen and Potsdam datasets are widely used for\n",
      "Former 76 DCSwin 77 BANet 78 EdgeViT 79 and CMT 80 the task of semantic segmentation of remotely sensed images so we\n",
      "These networks are deep learning networks that have achieved state conduct extensive experiments on these two datasets to validate the\n",
      "oftheart In order to make a fair comparison between the latest effectiveness of our approach The test results of SRSEN and segmen\n",
      "lightweight ViT models we utilize these networks as the backbone and tation networks with interpolated upsampling on the ×4 LR Vaihingen\n",
      "attach a simple segmentation head at the network’s end to construct the datasetarepresentedinTable1ItcanbeseenthattheproposedSRSEN\n",
      "segmentation network More importantly to ensure the output of HR outperforms the comparison network on all metrics Specifically in\n",
      "7L Wang et al Displays 84 2024 102800\n",
      "Fig 8 Visualization results of the LoveDa test set The first column denotes the HR RGB images The second column represents the input LR images GT denotes ground truth\n",
      "Fig 9 SR visualization results for the LR Vaihingen dataset Compared to bicubic interpolation SRSEN is able to recover a clearer image CFSA and MFRB also play a positive\n",
      "role in SR effects\n",
      "Table 5\n",
      "Comparison of segmentation accuracy by SRSEN with different downsampling scales of encoder on the LR\n",
      "Potsdam dataset\n",
      "Scale Impsurf Building Lowveg Tree Car mF1 OA mIoU\n",
      "4 7988 8349 7192 6971 8428 7926 7811 6734\n",
      "8 8353 8703 7596 7349 8652 8158 7929 6932\n",
      "16 8429 8740 7688 7452 8595 8211 8012 6999\n",
      "32 8148 8514 7285 7097 8456 7978 7761 6817\n",
      "comparison with the lightweight networks SRSEN outperforms UN advanced networks are limited when facing LR remote sensing images\n",
      "etFormer CMT and EdgeNet in terms of mIoU by 67 72 and and SRSEN with SR module can effectively improve the segmentation\n",
      "96 respectively SRSEN also improved mIoU and mF1 by 85 and accuracy of blurred images\n",
      "73 respectively compared to CNNbased BANet Moreover while the To verify the effectiveness of SR in SRSEN we visualize the feature\n",
      "segmentation accuracy of transformerbased DCSwin is slightly higher maps in networks at the LR Vaihingen dataset to compare The feature\n",
      "than SRSEN for the Building and Lowveg classes it is significantly maps of the intermediate positions of the different networks were\n",
      "lower than SRSEN for the other classes This indicates that these output for comparison As shown in Fig 5 Each column represents\n",
      "8L Wang et al Displays 84 2024 102800\n",
      "Fig 10 Segmentation results of SRSEN on different LR Potsdam datasets ×2×4×8 denote the scales of downsampling of the input image The accuracy of segmentation decreases\n",
      "as the resolution of the image decreases\n",
      "Fig 11 LAM attribution results for ×4 SR effects of different networks on the LR Vaihingen dataset There is a yellow box on the HR graph and LAM analyzed the importance\n",
      "of each pixel of the LR image in the model for the yellow box SR results The comparison shows that SRSEN is able to perceive more pixels of information\n",
      "the feature maps of three random channels in the network The com improves by up to 93 and the mF1 by up to 74 These results\n",
      "parison of feature maps shows that SRSEN exploits the SR method show that SRSEN achieves optimal segmentation accuracy on the LR\n",
      "to reconstruct features that contain more texture information which Potsdam dataset On LR remote sensing images the segmentation ac\n",
      "plays a crucial role in generating more accurate segmentation results\n",
      "curacy of lightweight EdgeViT is dramatically suppressed Although the\n",
      "In addition several predictions of the LR Vaihingen on different models\n",
      "segmentation accuracy of DCSwin with a larger number of parameters\n",
      "are shown in Fig 6 It can be seen that the shape of the buildings in\n",
      "is higher than that of lightweight networks it is still lower than SRSEN\n",
      "the segmentation results of SRSNR is closer to groundtrue And the\n",
      "with the SR module In addition the visual comparisons of networks\n",
      "segmentation of the car is more precise Thus the proposed SRSEN\n",
      "on the LR Potsdam dataset are shown in Fig 7 It can be seen that the\n",
      "generates more precise segmentation results on LR remote sensing\n",
      "segmentation effect of SRSEN is more accurate in various categories\n",
      "images\n",
      "For the ×4 LR Potsdam dataset Table 2 demonstrates the test Although DCSwin and SRSEN have similar segmentation results for\n",
      "results of the different models SRSEN achieves 6999 mIoU 8211 Building the segmentation accuracy of SRSEN for Tree is much higher\n",
      "mF1 and 8012 OA Compared to other models the mIoU of SRSEN than that of DCSwin\n",
      "9L Wang et al Displays 84 2024 102800\n",
      "Table 6 Table 7\n",
      "Ablation studies of CFSA and MFRB on LR Potsdam dataset The SR scale is 4 The Comparison of segmentation effects of different attention mechanisms\n",
      "complexity and speed are measured by the 256 × 256 input on a single NVIDIA GTX\n",
      "Attention Params mIoU mF1\n",
      "3090\n",
      "CFSA 88M 7015 8212\n",
      "CFSA MFRB ParametersM SpeedFPS mIoU mF1 PSNR SSIM\n",
      "NonLocal Attention 85M 6691 7824\n",
      "✓ ✓ 88 16 7015 8212 3303 0912 SelfAttention 85M 6935 8071\n",
      "✓ 86 18 6999 8210 3280 0906 CBAM 86M 6408 7716\n",
      "✓ 97 17 6813 8075 3224 0896\n",
      "95 15 6801 8053 3165 0887\n",
      "module CBAM The comparison results are shown in Table 7 and it\n",
      "can be seen that CFSA achieves optimal results at the cost of increasing\n",
      "45 Results on the LoveDA dataset\n",
      "the number of parameters by a smaller amount\n",
      "In the decoder the features are upsampled and then fused with\n",
      "We experimented with the ×4 LR LoveDA dataset to further eval\n",
      "the same scale features provided by the encoder to ensure the integrity\n",
      "uate the performance of SRSEN The comparison results are listed in\n",
      "of the lowfrequency information in the image Since simple feature\n",
      "Table 3 SRSEN achieves 4908 mIoU 6735 mF1 and 6338 OA\n",
      "fusion cannot achieve the desired results the MFRB is designed to filter\n",
      "Remarkably compared to recent lightweight models with interpolated\n",
      "and refine the fused features to improve the segmentation accuracy\n",
      "upsampling although CMT achieved the highest accuracy in classes\n",
      "We designed ablation experiments to verify the effectiveness of the two\n",
      "Background and Agricultural the accuracy of mF1 OA and mIoU in\n",
      "modules in the segmentation task\n",
      "SRSEN are all the highest It can be seen that the accuracy of the Forest\n",
      "In the ablation experiments we replaced CFSA with original self\n",
      "is extremely low on each model due to the LR images In addition the\n",
      "attention and MFRB with normal 3 × 3 convolution In addition we\n",
      "visual comparisons of networks on the LR LoveDA dataset are shown\n",
      "included the SR experiment to verify the effect of the two modules\n",
      "in Fig 8 It can be seen that due to the low resolution of the image the\n",
      "on the SR effect We evaluate the SR results by the peak signalto\n",
      "networks make quite a few mistakes and omissions in the segmentation\n",
      "noise ratio PSNR and the structural similarity index metrics SSIM\n",
      "task But SRSEN has the closest segmentation result to the ground truth\n",
      "on the Y channel only in the converted YCbCr space The comparison\n",
      "For a more comprehensive comparison of the networks’ perfor\n",
      "results of the experiments are presented in Table 6 It can be seen that\n",
      "mance Table 4 shows the segmentation accuracies mIoU and mF1\n",
      "CFSA significantly improves the segmentation accuracy and reduces\n",
      "of the networks on three LR datasets SRSEN achieves the highest seg\n",
      "the computational time compared to the original selfattention MFRB\n",
      "mentationaccuracyonalldatasetsandhasalownumberofparameters\n",
      "also improves the performance of the network by about 2 with the\n",
      "However the FPS of SRSEN is too low because the complex SR module\n",
      "02𝑀 number of parameters The SR visual comparison of the network\n",
      "inside SRSEN operates much slower than the interpolation method The\n",
      "is shown in Fig 9 SRSEN can significantly improve the resolution of\n",
      "SR module also makes SRSEN have a large amount of computation\n",
      "remote sensing images The SR comparison results indicate that CFSA\n",
      "Because the network is constantly generating higher resolution feature\n",
      "and MFRB have a positive effect on the SR capability of the network\n",
      "maps at runtime processing these feature maps takes up more compu\n",
      "We also explored the effect of upsampling scales of 2 4 and 8\n",
      "tational resources So the next step in the research is how to increase\n",
      "for SRSEN on the LR Potsdam dataset The corresponding training and\n",
      "the speed of the network with the SR model\n",
      "test datasets are downsampled 2 4 and 8 times respectively And\n",
      "the image sizes of the input networks are 512256128 respectively\n",
      "46 Ablation study The comparison results are presented in Table 8 Compared to the net\n",
      "work with only interpolated upsampling SRSEN with the SR module\n",
      "In order to separately evaluate the performance of each component has higher segmentation accuracy on downsampled images at three\n",
      "proposed in SRSEN we conduct a series of ablation experiments on the different scales SRSEN is most effective with ×4 LR datasets and its\n",
      "LR remote sensing datasets mIoU is about 65 higher than that of UNetFormer The network\n",
      "Multiscale feature encoder In the encoder of SRSEN to ex with SR module can better cope with the task of segmentation in LR\n",
      "tract deep semantic features from the image the downsampling path remote sensing images The segmentation effects of SRSEN on Potsdam\n",
      "reduces the resolution of the feature map layer by layer with a down with different upsampling scales are shown in Fig 10 SRSEN protects\n",
      "sampling scale of 2 for each layer The small scale of downsampling the accuracy of segmentation precision although the resolution of the\n",
      "will lead to the semantic features in the image being difficult to extract image is drastically reduced\n",
      "while a large downsampling scale will affect the recovery of features In addition we utilized local attribution maps LAM 81 to vali\n",
      "such as edges and textures in the decoder thus reducing segmentation date the performance of SRSEN on the ×4 SR task We compare SRSEN\n",
      "accuracy Since the inputs to the network are LR images we performed with the classical SR network RCAN 59 on the Vaihingen dataset The\n",
      "ablation experiments to explore the appropriate downsampling scale results are shown in Fig 11 When reconstructing patches marked with\n",
      "We set downsampling scales of 4 8 16 and 32 by varying the number yellow boxes a higher diffusion index DI indicates a larger scope of\n",
      "of RSAB in the encoder for experimental comparisons The experimen contextual information involved and a darker color indicates a larger\n",
      "tal results are presented in Table 5 It can be seen that the segmentation contribution The comparisons show that SRSEN can make full use of\n",
      "accuracy of the network is similar when the downsampling scale is the global information to complete the SR task which indicates that\n",
      "16 or 8 but the segmentation accuracy decreases substantially when SRSEN can effectively extract local information while exploring the\n",
      "the downsampling scale is 32 We conclude that downsampling the association of remote information\n",
      "input LR image with overlarge scale leads to the loss of usable texture\n",
      "features which makes it difficult for the model to recover the details 5 Conclusion\n",
      "of the original image and reduces the segmentation accuracy\n",
      "Superresolution fusion decoder The decoder of SRSEN exploits In this paper we explore the impact of LR remote sensing images on\n",
      "CFSA to establish remote dependencies of features more efficiently and semantic segmentation The multiorder SR semantic segmentation net\n",
      "avoid the loss of associations between local information In order to work SRSEN is designed based on the fusion of CNN and transformer\n",
      "verify the effectiveness of CFSA several attentional mechanisms were which improves the segmentation accuracy of LR remote sensing im\n",
      "substituted into the network for segmentation experiments including ages by utilizing the SR method The encoder of SRSEN contains\n",
      "nonlocal attention selfattention and convolutional block attention two paths the downsampling path generates deep semantic features\n",
      "10L Wang et al Displays 84 2024 102800\n",
      "Table 8 References\n",
      "Comparison of segmentation accuracies for different upsampling scales of SRSEN and\n",
      "UNetFormer on the LR Potsdam dataset\n",
      "1 R Li S Zheng C Duan Land cover classification from remote sensing images\n",
      "Methods Scale mIoU mF1 OA based on multiscale fully convolutional network GeoSpat Inf Sci GeoSpat\n",
      "SRSEN 2 7521 8698 8533 Inf Sci 2020\n",
      "4 6999 8211 8012 2 E Maggiori Y Tarabalka G Charpiat P Alliez Convolutional neural networks\n",
      "8 6216 7435 7271 for largescale remotesensing image classification IEEE Trans Geosci Remote\n",
      "Sens 2017 645–657\n",
      "UNetFormer 2 7308 8565 8381\n",
      "3 D Marcos M Volpi B Kellenberger D Tuia Land cover mapping at very high\n",
      "4 6326 7787 8060\n",
      "resolution with rotation equivariant CNNs Towards small yet accurate models\n",
      "8 5936 7183 7047\n",
      "ISPRS J Photogramm Remote Sens 2018 96–107\n",
      "4 Y Wang Y Tian Exploiting multiscale contextual prompt learning for zeroshot\n",
      "semantic segmentation Displays 81 2024 102616\n",
      "5 F Lu T Liu T Zhang B Jin W Gu CHDNet A lightweight weakly supervised\n",
      "through convolutional groups and the upsampling path provides the\n",
      "segmentation network for lung CT image Displays 82 2024 102650\n",
      "decoder with HR features containing rich lowfrequency information 6 Q Zhang J Tang H Zheng C Lin Efficient object detection method based on\n",
      "through subpixel convolution The decoder of SRSEN utilizes SRTB aerial optical sensors for remote sensing Displays 75 2022 102328\n",
      "7 G Chen L Li J Zhang Y Dai Rethinking the unpretentious UNet for medical\n",
      "to extract deep semantic information while reconstructing HR fea\n",
      "ultrasound image segmentation Pattern Recognit 142 2022 109728\n",
      "tures SRTB adopts the transformer structure and replaces the original\n",
      "8 G Chen L Li Y Dai J Zhang MH Yap AAUNet An adaptive attention UNet\n",
      "selfattention with CFSA CFSA achieves more efficient establishment for breast lesions segmentation in ultrasound images IEEE Trans Med Imaging\n",
      "of remote dependencies of features while avoiding the loss of local 42 5 2023 1289–1300\n",
      "9 Y Li T Shi Y Zhang W Chen Z Wang H Li Learning deep semantic\n",
      "associations with the help of convolutional operations and channel\n",
      "segmentation network under multiple weaklysupervised constraints for cross\n",
      "attention mechanism As there are a large number of feature fusions\n",
      "domain remote sensing image semantic segmentation ISPRS J Photogramm\n",
      "in the network the fused features at the tail of the decoder are refined Remote Sens 175 2021 20–33\n",
      "and filtered by MFRB to further improve the segmentation accuracy 10 D Hao B Xia W Liu Z Zhang J Zhang X Wang S Xu A novel mamba\n",
      "architecture with a semantic transformer for efficient realtime remote sensing\n",
      "A comprehensive set of benchmark experiments and ablation studies\n",
      "semantic segmentation Remote Sensing 16 2024 2620\n",
      "on the ISPRS Vaihingen and Potsdam datasets as well as the LoveDA\n",
      "11 Z Dong W Qiong Z Jinglin B Cong Mine diversified contents of multispectral\n",
      "dataset demonstrated the effectiveness and efficiency of the proposed cloud images along with geographical information for multilabel classification\n",
      "method IEEE Trans Geosci Remote Sens 2023 1–15\n",
      "12 S Shuyao Z Jinglin W Xing Faster and lighter meteorological satellite image\n",
      "The inference speed of the model receives a hindrance due to the\n",
      "classification by a lightweight channeldilationconcatenation net IEEE Journal\n",
      "incorporation of SR methods In the next step we will conduct further of Selected Topics in Applied Earth Observations and Remote Sensing 2023\n",
      "research on the inference speed of the model and explore more efficient 2301–2317\n",
      "13 Y Cai Y Yang Y Shang Z Shen J Yin DASRSNet Multitask domain\n",
      "LR image segmentation models\n",
      "adaptation for superresolutionaided semantic segmentation of remote sensing\n",
      "images IEEE Trans Geosci Remote Sens 2023 1–18\n",
      "14 Z Tang S Yan C Xu Adaptive superresolution image reconstruction based on\n",
      "CRediT authorship contribution statement\n",
      "fractal theory Displays 80 2023 102544\n",
      "15 D Chao CL Chen H Kaiming T Xiaoou Image superresolution using deep\n",
      "convolutional networks IEEE Trans Pattern Anal Mach Intell 99 2016 1–307\n",
      "Lunqian Wang Methodology Writing – original draft Software\n",
      "16 B Lim S Son H Kim S Nah KM Lee Enhanced deep residual networks\n",
      "Validation Writing – review  editing Xinghua Wang Conceptualiza\n",
      "for single image superresolution in European Conference on Computer Vision\n",
      "tion Validation Visualization Weilin Liu Formal analysis Resources 2017 pp 1132–1140 abs170702921\n",
      "Data curation Hao Ding Supervision Writing – original draft Bo Xia 17 H Muhammad S Greg U Norimichi G Shakhnarovich Deep backprojection\n",
      "networks for superresolution in European Conference on Computer Vision\n",
      "Formal analysis Data curation Zekai Zhang Investigation Resources\n",
      "2018 pp 1664–1673 abs180302735\n",
      "Software Jinglin Zhang Conceptualization Data curation Writing –\n",
      "18 Y Zhang K Li K Li L Wang B Zhong Y Fu Image superresolution using\n",
      "review  editing Supervision Project administration Sen Xu Data very deep residual channel attention networks Comput Vis  ECCV 11211\n",
      "curation Supervision Project administration 2018 294–310\n",
      "19 J Liang J Cao G Sun K Zhang LV Gool R Timofte Swinir Image\n",
      "restoration using swin transformer in IEEECVF International Conference on\n",
      "Declaration of competing interest Computer Vision Workshops 2021 pp 1833—1844\n",
      "20 X Chen X Wang J Zhou Y Qiao C Dong Activating more pixels in image\n",
      "superresolution transformer 2023 arXiv220504437\n",
      "The authors declare that they have no known competing finan 21 J Shi H Li T Liu Y Liu M Zhang J Zhu L Zheng S Weng Image\n",
      "superresolution using efficient striped window transformer 2023 arXiv2301\n",
      "cial interests or personal relationships that could have appeared to\n",
      "09869\n",
      "influence the work reported in this paper\n",
      "22 Z Hui X Gao Y Yang X Wang Lightweight image superresolution with\n",
      "information multidistillation network in ACM International Conference on\n",
      "Multimedia 2019 pp 2024–2032\n",
      "Data availability\n",
      "23 Y Wenming W Wei Z Xuechen S Shuifa L Qingmin Lightweight feature\n",
      "fusion network for single image superresolution IEEE Signal Process Lett 26\n",
      "2019 538–542\n",
      "Data will be made available on request\n",
      "24 X Luo Q Liang D Liu Y Qu Boosting lightweight single image super\n",
      "resolution via jointdistillation in ACM International Conference on Multimedia\n",
      "2021 pp 1535–1543\n",
      "Acknowledgments\n",
      "25 SGuoLLiuZGanYWangWZhangCWangGJiangWZhangRYiL\n",
      "Ma K Xu ISDNet Integrating shallow and deep networks for efficient ultrahigh\n",
      "resolution segmentation in CVPR in 1 Vol 2022 2022 pp 4351–4360\n",
      "This work was supported in part by the Key Research and Develop\n",
      "26 W Shi F Jiang D Zhao Single image superresolution with dilated convolution\n",
      "ment Program of Shandong Province under Grant 2023CXGC010112 based multiscale information learning inception module Comput Res Repos\n",
      "Distinguished Young Scholar of Shandong Province under Grant 2017 977–981 abs170707128\n",
      "27 J Li F Fang J Li K Mei G Zhang MDCN Multiscale dense cross network\n",
      "ZR2023JQ025 Taishan Scholars Program under Grant tsqn202211290\n",
      "for image superresolution Comput Res Repos 31 2021 2547–2561\n",
      "Major Basic Research Projects of Shandong Province under Grant\n",
      "28 Q Yang Y Liu J Yang Twobranch crisscross network for realistic and accurate\n",
      "ZR2022ZD32 and Jiangsu University Qing Lan Project image superresolution Displays 80 2023 102549\n",
      "11L Wang et al Displays 84 2024 102800\n",
      "29 SAA Kohl B RomeraParedes KH MaierHein DJ Rezende SMA Eslami 54 G Wang M Chen YC Lin X Tan C Zhang W Yao B Gao K Li Z Li\n",
      "P Kohli A Zisserman O Ronneberger A hierarchical probabilistic UNet for W Zeng Efficient multibranch dynamic fusion network for superresolution of\n",
      "modeling multiscale ambiguities Comput Res Repos 2019 abs190513077 industrial component image Displays 82 2024 102633\n",
      "30 T Andrew S Karan C Bryan Hierarchical multiscale attention for semantic 55 N Han L Zhou Z Xie J Zheng L Zhang Multilevel UNet network for image\n",
      "segmentation 2020 CoRR abs200510821 superresolution reconstruction Displays 73 2022 102192\n",
      "31 J Gu H Kwon D Wang W Ye M Li YH Chen L Lai V Chandra DZ Pan 56 R Xu X Kang C Li H Chen A Ming DCTFANet DCT based frequency\n",
      "HRViT Multiscale highresolution vision transformer 2021 arXiv211101236 attention network for single image superresolution Displays 74 2022 102220\n",
      "32 J Long E Shelhamer T Darrell Fully convolutional networks for semantic 57 C Dong CC Loy X Tang Accelerating the superresolution convolutional\n",
      "segmentation in CVPR 2015 neural network in COMPUTER VISION  ECCV 2016 PT II Vol 9906 2016\n",
      "33 R Kemker C Salvaggio C Kanan Algorithms for semantic segmentation of pp 391–407\n",
      "multispectral remote sensing imagery using deep learning ISPRS J Photogramm 58 W Shi J Caballero F Huszár J Totz AP Aitken R Bishop D Rueckert\n",
      "Remote Sens 2018 60–77 Z Wang Realtime single image and video superresolution using an efficient\n",
      "34 R Kemker C Salvaggio C Kanan Algorithms for semantic segmentation of subpixel convolutional neural network in CVPR 2016 pp 1874–1883 abs\n",
      "multispectral remote sensing imagery using deep learning ISPRS J Photogramm 160905158\n",
      "Remote Sens 2018 60–77 59 Z Yulun L Kunpeng L Kai W Lichen Z Bineng F Yun Transformer\n",
      "35 XY Tong GS Xia Q Lu H Shen S Li S You L Zhang Landcover based decoder designs for semantic segmentation on remotely sensed images\n",
      "classification with highresolution remote sensing images using transferable deep in European Conference on Computer Vision Vol 11211 2018 pp 294–310\n",
      "models Remote Sens Environ 2020 111322 60 Y Mei Y Fan Y Zhou Image superresolution with nonlocal sparse attention\n",
      "36 O Ronneberger P Fischer T Brox UNet Convolutional networks for biomed in Computer Vision and Pattern Recognition 2021 pp 3516–3525\n",
      "ical image segmentation Lect Notes Comput Sci Lect Notes Comput Sci 61 Z Liu Y utong Lin Y ue Cao H Hu Y Wei Z Zhang S Lin B Guo\n",
      "2015 Swin transformer Hierarchical vision transformer us ing shifted windows in\n",
      "37 V Badrinarayanan A Kendall R Cipolla SegNet A deep convolutional encoder IEEECVF International Conference on Computer Vision 2021 pp 9992–10002\n",
      "decoder architecture for image segmentation IEEE Trans Pattern Anal Mach 62 Z Zhanpeng Z Kaipeng FarSeeNet Realtime semantic segmentation by effi\n",
      "Intell 2017 2481–2495 cient multiscale context aggregation and feature space superresolution in IEEE\n",
      "38 LC Chen Y Zhu G Papandreou F Schroff H Adam Encoderdecoder with International Conference on Robotics and Automation 2020 pp 8411–8417\n",
      "atrous separable convolution for semantic image segmentation in Computer 63 J Jiang J Liu J Fu W Wang H Lu Superresolution semantic segmentation\n",
      "Vision – ECCV 2018 in Lecture Notes in Computer Science 2018 pp 833–851 with relation calibrating network Pattern Recognit 124 2022 108501\n",
      "39 Y Shen J Chen L Xiao D Pan Optimizing multiscale segmentation with local 64 L Wang D Li Y Zhu L Tian Y Shan Dual superresolution learning for\n",
      "spectral heterogeneity measure for high resolution remote sensing images ISPRS semantic segmentation in CVPR 2020 pp 3773–3782\n",
      "J Photogramm Remote Sens 2019 13–25 65 C Xiang W Wang L Deng P Shi X Kong Crack detection algorithm for\n",
      "40 D Marmanis K Schindler J Wegner S Galliani M Datcu U Stilla Classi concrete structures based on superresolution reconstruction and segmentation\n",
      "fication with an edge Improving semantic image segmentation with boundary network Autom Constr 140 2022 104346\n",
      "detection Isprs J Photogr Remote Sens Isprs J Photogr Remote Sens 2016 66 Q Delannoy CH Pham C Cazorla C TorDíez G Dollé H Meunier N\n",
      "41 K Nogueira M Dalla Mura J Chanussot WR Schwartz JA dos San Bednarek R Fablet N Passat F Rousseau SegSRGAN Superresolution and\n",
      "tos Dynamic multicontext segmentation of remote sensing images based on segmentation using generative adversarial networks  application to neonatal\n",
      "convolutional networks IEEE Trans Geosci Remote Sens 2019 7503–7520 brain MRI Comput Biol Med 120 2020 103755\n",
      "42 D Marcos M Volpi B Kellenberger D Tuia Land cover mapping at very high 67 H Wu B Xiao N Codella M Liu X Dai L Yuan L Zhang CvT Introducing\n",
      "resolution with rotation equivariant CNNs Towards small yet accurate models Convolutions to Vision Transformers 2021 pp 22–31\n",
      "ISPRS J Photogramm Remote Sens 2018 96–107 68 TXiaoMSinghEMintunTDarrellPDollárRGirshickEarlyconvolutions\n",
      "43 A Dosovitskiy L Beyer A Kolesnikov D Weissenborn X Zhai T Unterthiner help transformers see better in Conference on Neural Information Processing\n",
      "M Dehghani M Minderer G Heigold S Gelly J Uszkoreit N Houlsby An Systems 2021 pp 30392–30400\n",
      "image is worth 16x16 words Transformers for image recognition at scale 2020 69 K Yuan S Guo Z Liu A Zhou F Yu W Wu Incorporating Convolution\n",
      "arXiv Computer Vision and Pattern Recognition arXiv Computer Vision and Designs into Visual Transformers 2021 pp 559–568\n",
      "Pattern Recognition 70 G Wu J Jiang Y Bai X Liu Incorporating transformer designs into convo\n",
      "44 R Strudel R Garcia I Laptev C Schmid Segmenter Transformer for semantic lutions for lightweight image superresolution IEEE Geosci Remote Sens Lett\n",
      "segmentation in 2021 IEEECVF International Conference on Computer Vision 2023 1–5 abs230314324\n",
      "ICCV 2021 71 L Zheng J Zhu J Shi S Weng Efficient mixed transformer for single image\n",
      "45 L Wang R Li C Duan C Zhang X Meng S Fang A novel transformer based superresolution 2023 CoRR abs230511403\n",
      "semantic segmentation scheme for fineresolution remote sensing images IEEE 72 RPK Poudel U Bonde S Liwicki C Zach ContextNet Exploring context\n",
      "Geosci Remote Sens Lett IEEE Geosci Remote Sens Lett 2021 and detail for semantic segmentation in realtime in British Machine Vision\n",
      "46 L Wang R Li D Wang C Duan T Wang X Meng Transformer meets Conference 2018 abs180504554\n",
      "convolution A bilateral awareness network for semantic segmentation of very 73 RPK Poudel S Liwicki R Cipolla FastSCNN Fast semantic segmentation\n",
      "fine resolution urban scene images Remote Sens 2021 3065 network in British Machine Vision Conference 2019 p 289 abs190204502\n",
      "47 S Zheng J Lu H Zhao X Zhu Z Luo Y Wang Y Fu J Feng T Xiang PH 74 C Yu J Wang C Peng C Gao G Yu N Sang Bisenet Bilateral Segmentation\n",
      "Torr L Zhang Rethinking semantic segmentation from a sequencetosequence Network For RealTime Semantic Segmentation Vol 11217 2018 pp 334–349\n",
      "perspective with transformers in CVPR 2021 arXiv Computer Vision and Pattern Recognition\n",
      "48 T Panboonyuen K Jitkajornwanich S Lawawirojwong P Srestasathiern P 75 J Wang Z zheng A Ma X Lu Y Zhong LoveDA A remote sensing landcover\n",
      "Vateekul Transformerbased decoder designs for semantic segmentation on dataset for domain adaptive semantic segmentation in Conference on Neural\n",
      "remotely sensed images Remote Sens 2021 5100 Information Processing Systems 2021 abs211008733\n",
      "49 K Umehara J Ota T Ishida Application of superresolution convolutional 76 L Wang R Li C Zhang S Fang C Duan X Meng PM Atkinson UNetFormer\n",
      "neural network for enhancing image resolution in chest CT J Digit Imaging A unetlike transformer for efficient semantic segmentation of remote sensing\n",
      "31 4 2018 441–450 urban scene imagery ISPRS J Photogramm Remote Sens 190 2022 196–214\n",
      "50 X Zhao Y Zhang T Zhang X Zou Channel splitting network for single MR 77 L Wang R Li C Duan S Fang A novel transformer based semantic segmen\n",
      "image superresolution IEEE Trans Image Process 28 11 2019 5649–5662 tation scheme for fineresolution remote sensing images IEEE Geosci Remote\n",
      "51 C Tuna GB Ünal E Sertel Singleframe super resolution of remotesensing Sens Lett 19 2022 1–5\n",
      "images by convolutional neural networks Int J Remote Sens 39 8 2018 78 L Wang R Li D Wang C Duan T Wang X Meng Transformer meets\n",
      "2463–2479 convolution A bilateral awareness network for semantic segmentation of very\n",
      "52 W Xu X Guangluan Y Wang X Sun D Lin W Yirong High quality fine resolution urban scene images Remote Sens 13 2021 3065\n",
      "remote sensing image superresolution using deep memory connected network 79 J Pan A Bulat F Tan X Zhu L Dudziak H Li G Tzimiropoulos B\n",
      "in IGARSS 20182018 IEEE International Geoscience and Remote Sensing Martinez EdgeViTs Competing lightweight CNNs on mobile devices with vision\n",
      "Symposium 2018 pp 8889–8892 transformers Lecture Notes in Comput Sci 13671 2022 294–311\n",
      "53 YD Wang R Armstrong P Mostaghimi Super resolution convolutional neural 80 J Guo K Han H Wu C Xu Y Tang C Xu Y Wang CMT Convolutional\n",
      "network models for enhancing resolution of rock microCT images 2019 arXiv neural networks meet vision transformers in CVPR 2021 pp 12165–12175\n",
      "Computer Vision and Pattern Recognition abs190407470 abs210706263\n",
      "81 J Gu C Dong Interpreting superresolution networks with local attribution\n",
      "maps in CVPR 2021 pp 9199–9208\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Menghilangkan tanda baca\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Remove punctuation\n",
    "teks_1_clean = teks_1.translate(translator)\n",
    "print(teks_1_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598c44d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsevier.com/locate/displa\n",
      "✩\n",
      "FSNet: A dual-domain network for few-shot image classification\n",
      "∗\n",
      "Xuewen \n"
     ]
    }
   ],
   "source": [
    "# Artikel rekomendasi dari science direct\n",
    "teks_2 = ekstrak_teks_dari_pdf(\"./jurnal/paper2.pdf\")\n",
    "print(teks_2[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b279d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation\n",
      "Jiaqi Gu2*, Hyoukjun Kwon1,\n"
     ]
    }
   ],
   "source": [
    "# Artikel yang disitasi paper ini\n",
    "teks_3 = ekstrak_teks_dari_pdf(\"./jurnal/paper3.pdf\")\n",
    "print(teks_3[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22f12fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e at ScienceDirect\n",
      "Engineering Applications of Artificial Intelligence\n",
      "journal homepage: www.elsevie\n"
     ]
    }
   ],
   "source": [
    "# Artikel dengan author yang sama (jurnal beda)\n",
    "teks_4 = ekstrak_teks_dari_pdf(\"./jurnal/paper4.pdf\")\n",
    "print(teks_4[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede7d634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mepage: www.elsevier.com/locate/neucom\n",
      "A multimodal fusion framework for semantic segmentation of re\n"
     ]
    }
   ],
   "source": [
    "# Artikel yang menyitasi paper ini\n",
    "teks_5 = ekstrak_teks_dari_pdf(\"./jurnal/paper5.pdf\")\n",
    "print(teks_5[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06579a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b013cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "132c4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(ngrams1, ngrams2):\n",
    "    c1, c2 = Counter(ngrams1), Counter(ngrams2)\n",
    "    all_keys = set(c1.keys()) | set(c2.keys())\n",
    "    \n",
    "    dot = sum(c1[k] * c2[k] for k in all_keys)\n",
    "    norm1 = math.sqrt(sum(v**2 for v in c1.values()))\n",
    "    norm2 = math.sqrt(sum(v**2 for v in c2.values()))\n",
    "    \n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b49961c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine_sim_sklearn\n",
    "\n",
    "def cek_n_gram_all(text_1, text_2):\n",
    "    tokens1 = tokenize(text_1)\n",
    "    tokens2 = tokenize(text_2)\n",
    "\n",
    "    hasil = {}\n",
    "    for n in [1, 2, 3]:\n",
    "        # Cosine dengan n-gram manual\n",
    "        ngrams1 = ngrams(tokens1, n)\n",
    "        ngrams2 = ngrams(tokens2, n)\n",
    "        cosine = cosine_similarity(ngrams1, ngrams2)\n",
    "        hasil[f\"Cosine_n{n}\"] = cosine\n",
    "\n",
    "        # Jaccard\n",
    "        set1, set2 = set(ngrams1), set(ngrams2)\n",
    "        jaccard = len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "        hasil[f\"Jaccard_n{n}\"] = jaccard\n",
    "\n",
    "        # TF-IDF + Cosine\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(n, n))\n",
    "        tfidf = vectorizer.fit_transform([text_1, text_2])\n",
    "        tfidf_cosine = cosine_sim_sklearn(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "        hasil[f\"TFIDF_Cosine_n{n}\"] = tfidf_cosine\n",
    "\n",
    "    return hasil\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae408d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n                                      Perbandingan    Cosine   Jaccard  \\\n",
      "0   1                                Dua file yang sama  1.000000  1.000000   \n",
      "1   1  Artikel yang direkomendasikan oleh ScienceDirect  0.886931  0.218020   \n",
      "2   1                   Artikel yang disitasi paper ini  0.794914  0.188248   \n",
      "3   1                   Artikel dengan author yang sama  0.892828  0.285790   \n",
      "4   1                  Artikel yang menyitasi paper ini  0.895501  0.236964   \n",
      "5   2                                Dua file yang sama  1.000000  1.000000   \n",
      "6   2  Artikel yang direkomendasikan oleh ScienceDirect  0.404677  0.058105   \n",
      "7   2                   Artikel yang disitasi paper ini  0.223014  0.037941   \n",
      "8   2                   Artikel dengan author yang sama  0.401472  0.091966   \n",
      "9   2                  Artikel yang menyitasi paper ini  0.463394  0.070110   \n",
      "10  3                                Dua file yang sama  1.000000  1.000000   \n",
      "11  3  Artikel yang direkomendasikan oleh ScienceDirect  0.081694  0.015632   \n",
      "12  3                   Artikel yang disitasi paper ini  0.025097  0.007158   \n",
      "13  3                   Artikel dengan author yang sama  0.106152  0.035355   \n",
      "14  3                  Artikel yang menyitasi paper ini  0.126176  0.023749   \n",
      "\n",
      "    TFIDF_Cosine  \n",
      "0       1.000000  \n",
      "1       0.861000  \n",
      "2       0.767788  \n",
      "3       0.896744  \n",
      "4       0.881870  \n",
      "5       1.000000  \n",
      "6       0.279095  \n",
      "7       0.143581  \n",
      "8       0.341229  \n",
      "9       0.357678  \n",
      "10      1.000000  \n",
      "11      0.045963  \n",
      "12      0.013297  \n",
      "13      0.074408  \n",
      "14      0.075917  \n"
     ]
    }
   ],
   "source": [
    "perbandingan = [\n",
    "    (\"Dua file yang sama\", teks_1, teks_1),\n",
    "    (\"Artikel yang direkomendasikan oleh ScienceDirect\", teks_1, teks_2),\n",
    "    (\"Artikel yang disitasi paper ini\", teks_1, teks_3),\n",
    "    (\"Artikel dengan author yang sama\", teks_1, teks_4),\n",
    "    (\"Artikel yang menyitasi paper ini\", teks_1, teks_5),\n",
    "]\n",
    "\n",
    "hasil = []\n",
    "for n in [1, 2, 3]:\n",
    "    for label, t1, t2 in perbandingan:\n",
    "        # Menghilangkan tanda baca\n",
    "        # translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "        # t1 = t1.translate(translator)\n",
    "        # t2 = t2.translate(translator)\n",
    "\n",
    "        metrics = cek_n_gram_all(t1, t2)\n",
    "        hasil.append({\n",
    "            \"n\": n,\n",
    "            \"Perbandingan\": label,\n",
    "            \"Cosine\": metrics[f\"Cosine_n{n}\"],\n",
    "            \"Jaccard\": metrics[f\"Jaccard_n{n}\"],\n",
    "            \"TFIDF_Cosine\": metrics[f\"TFIDF_Cosine_n{n}\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(hasil)\n",
    "\n",
    "# Pastikan urut sesuai n lalu perbandingan\n",
    "df = df.sort_values(by=[\"n\"]).reset_index(drop=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da0d7207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Perbandingan",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Cosine",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Jaccard",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TFIDF_Cosine",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "24810826-77f6-411c-8fbc-078df88d794e",
       "rows": [
        [
         "0",
         "1",
         "Dua file yang sama",
         "1.0",
         "1.0",
         "1.0000000000000016"
        ],
        [
         "1",
         "1",
         "Artikel yang direkomendasikan oleh ScienceDirect",
         "0.8869313651338973",
         "0.21802002224694106",
         "0.8609996984942739"
        ],
        [
         "2",
         "1",
         "Artikel yang disitasi paper ini",
         "0.7949137816779133",
         "0.18824780643445893",
         "0.7677881025189596"
        ],
        [
         "3",
         "1",
         "Artikel dengan author yang sama",
         "0.8928276054363145",
         "0.28578989150568934",
         "0.8967441331709485"
        ],
        [
         "4",
         "1",
         "Artikel yang menyitasi paper ini",
         "0.8955007263967993",
         "0.23696437790397523",
         "0.8818696848997088"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Perbandingan</th>\n",
       "      <th>Cosine</th>\n",
       "      <th>Jaccard</th>\n",
       "      <th>TFIDF_Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dua file yang sama</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Artikel yang direkomendasikan oleh ScienceDirect</td>\n",
       "      <td>0.886931</td>\n",
       "      <td>0.218020</td>\n",
       "      <td>0.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Artikel yang disitasi paper ini</td>\n",
       "      <td>0.794914</td>\n",
       "      <td>0.188248</td>\n",
       "      <td>0.767788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Artikel dengan author yang sama</td>\n",
       "      <td>0.892828</td>\n",
       "      <td>0.285790</td>\n",
       "      <td>0.896744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Artikel yang menyitasi paper ini</td>\n",
       "      <td>0.895501</td>\n",
       "      <td>0.236964</td>\n",
       "      <td>0.881870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n                                      Perbandingan    Cosine   Jaccard  \\\n",
       "0  1                                Dua file yang sama  1.000000  1.000000   \n",
       "1  1  Artikel yang direkomendasikan oleh ScienceDirect  0.886931  0.218020   \n",
       "2  1                   Artikel yang disitasi paper ini  0.794914  0.188248   \n",
       "3  1                   Artikel dengan author yang sama  0.892828  0.285790   \n",
       "4  1                  Artikel yang menyitasi paper ini  0.895501  0.236964   \n",
       "\n",
       "   TFIDF_Cosine  \n",
       "0      1.000000  \n",
       "1      0.861000  \n",
       "2      0.767788  \n",
       "3      0.896744  \n",
       "4      0.881870  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcac6e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rlrrr}\n",
      "\\toprule\n",
      "n & Perbandingan & Cosine & Jaccard & TFIDF_Cosine \\\\\n",
      "\\midrule\n",
      "1 & Dua file yang sama & 1.0000 & 1.0000 & 1.0000 \\\\\n",
      "1 & Artikel yang direkomendasikan oleh ScienceDirect & 0.8869 & 0.2180 & 0.8610 \\\\\n",
      "1 & Artikel yang disitasi paper ini & 0.7949 & 0.1882 & 0.7678 \\\\\n",
      "1 & Artikel dengan author yang sama & 0.8928 & 0.2858 & 0.8967 \\\\\n",
      "1 & Artikel yang menyitasi paper ini & 0.8955 & 0.2370 & 0.8819 \\\\\n",
      "2 & Dua file yang sama & 1.0000 & 1.0000 & 1.0000 \\\\\n",
      "2 & Artikel yang direkomendasikan oleh ScienceDirect & 0.4047 & 0.0581 & 0.2791 \\\\\n",
      "2 & Artikel yang disitasi paper ini & 0.2230 & 0.0379 & 0.1436 \\\\\n",
      "2 & Artikel dengan author yang sama & 0.4015 & 0.0920 & 0.3412 \\\\\n",
      "2 & Artikel yang menyitasi paper ini & 0.4634 & 0.0701 & 0.3577 \\\\\n",
      "3 & Dua file yang sama & 1.0000 & 1.0000 & 1.0000 \\\\\n",
      "3 & Artikel yang direkomendasikan oleh ScienceDirect & 0.0817 & 0.0156 & 0.0460 \\\\\n",
      "3 & Artikel yang disitasi paper ini & 0.0251 & 0.0072 & 0.0133 \\\\\n",
      "3 & Artikel dengan author yang sama & 0.1062 & 0.0354 & 0.0744 \\\\\n",
      "3 & Artikel yang menyitasi paper ini & 0.1262 & 0.0237 & 0.0759 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_table = df.to_latex(index=False, float_format=\"%.4f\")\n",
    "\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
